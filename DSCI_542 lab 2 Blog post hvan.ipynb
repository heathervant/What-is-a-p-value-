{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Presumptuous P-value\n",
    "\n",
    "When conducting research, many people want to know if all of the hard work they put in and data that they collected means anything. Thus, in order to quantify the statistical significance of a researcher's findings, it has become common practice, although recently a more controversial one, to determine the reliability of an experiment by using a calculated probability known as the *p-value*. In order to demonstrate this concept, let's consider an example;\n",
    "\n",
    "Imagine you are a very conscientious nutritionist and want to know whether your female clients are on average eating more or less servings of fruits and vegetables (fruitables) per day than the average female citizen. Luckily, the government has already collected some data from a large sample of 1.3 million women and found that the average amount of fruitables eaten per day was 4.90, with a standard deviation of 0.1 [StatCan, 2002](https://www150.statcan.gc.ca/n1/en/pub/82-003-x/2001003/article/6103-eng.pdf?st=YqoR0ZlM). 25 of your 40 clients on average eat 6.08 servings per day. Let’s call this value your test statistic, as it is a statistic that you would like to test or compare. It doesn’t do much good to just eyeball these two averages and say, “Well obviously 6.08 is much higher than 4.90, so you are doing a great job, you deserve a raise”. In order to conduct an unbiased scientific test to see how unusual your results are, most statisticians will tell you to find out how unusual your results are, assuming that the average or null hypothesis is true using something called a hypothesis test and a p-value. Before I explain how to conduct a hypothesis test with this data and what a p-value is, and why you might want to use it, I will define a few important terms:\n",
    "\n",
    "- Definition of *null hypothesis*:  A hypothesis which states that there is no significant difference between specified populations, and any observed difference is due to sampling or experimental error. This statement is usually more specific and uninteresting than the alternate hypothesis, as it is expected under normal circumstances. Usually researchers are trying to disprove this hypothesis, as it is interesting to find support for something that is not yet known or expected.\n",
    "\n",
    "- Conversely, an *Alternate hypothesis* is not specific and is essentially a statement that describes the null hypothesis not happening, and is usually more interesting than the null hypothesis. The alternate hypothesis can be either *1-tailed* or *2-tailed* depending on if there are possible parameter values on one or both sides of the value specified by the null hypothesis respectively.\n",
    "\n",
    "In this case, your null hypothesis is that your clients are eating on average 4.9 servings of fruitables per day. Your alternate hypothesis would be that the population parameter for your 40 clients is not 4.9 servings of fruitables per day. Since it is possible for your clients to be eating more or less than the average, we can say that this is 2-tailed test.\n",
    "\n",
    "Now the tricky part is finding out what the null distribution is, which you need in order to carry out a hypothesis test. In order to do this, you can create a null distribution for your test statistic, by simulating a bunch of scenarios (how about 80!) with the same sample size of 25 people, and a mean of 4.9, and standard deviation of 0.1 (as this is what was found in the population). Then record the mean serving of fruitables consumed in each trial and display them with a probability distribution. Here is what those sample means from your simulated 80 trials might look like: In order to do this, you can create a null distribution for your test statistic, by simulating a bunch of scenarios (how about 80!) with the same sample size of 25 people, and a mean of 4.9, and standard deviation of 0.1 (as this is what was found in the population). Then record the mean serving of fruitables consumed in each trial and display them with a probability distribution. Here is what those sample means from your simulated 80 trials might look like:\n",
    "\n",
    "![fruitables](https://github.ubc.ca/MDS-2018-19/DSCI_542_lab2_hvan/blob/master/Fruitables.png?raw=true)\n",
    "\n",
    "In order to calculate your p-value for this experiment, you would take your test statistic value of 6.08 and see if it is as weird or weirder than getting the average result of 4.9 by using this normal distribution. Since it is a 2-tailed test this means finding the area under the normal distribution on either end of the curve that corresponds to being equal to or greater than 6.08. A computer can find an exact p-vale, but usually we can just look up the p-value in a table that mathemeticians have kindly generated for us. In this case our p-value is 0.0001. Depending on the significance level that the scientific community deems acceptable, we can determine whether to reject or fail to reject the null hypothesis. The most commonly used significance threshold $\\alpha$ is the p-value of 0.05. So if our p-value is less than the significance threshold of 0.05, this means that the probability of obtaining this test statistic assuming the null hypothesis is true is less than 5%. Thus, we can reject our null hypothesis and support the alternate hypothesis. \n",
    "\n",
    "\n",
    "## Common Misconception of the p-value\n",
    "The p-value tells you about the likelihood of obtaining these results assuming that the average or expected value is correct or true. Thus, it tells you about how good your test is at finding an unexpected result. It does not however tell you anything about  whether or not your sample is truly different than the what is expected due to chance, given the test statistic.\n",
    " \n",
    "\n",
    "## History of the p-value\n",
    "The p-value was originally invented to determine whether something is worth investigating further. The p-value was later popularized by  Ronald Fisher, who devised an approach called “significance testing” which he used to make inferences based on the evidence in the data. 8 years later, two other statisticians named Neyman and Pearson invented “hypothesis testing” which assumed that no experiment could provide evidence about any particular hypothesis and instead focused on minimising  wrong conclusions. These two different concepts can sometimes be confused, and is worth further reading.\n",
    "\n",
    "## Ethical use of p-values\n",
    "It is important to set the significance level before analyzing your results for ethical reasons. Otherwise, you may be tempted to change the significance level to be larger than your p-value to make your findings seem more interesting. Or worse, researchers may manipulate the data, which might involve leaving out some data points or asking a slightly different question, to obtain a significant result. This is known as p-hacking, and is frowned upon, as it decreases the integrity and reliability of findings. \n",
    "\n",
    "For further reading, I suggest this [article](http://theconversation.com/give-p-a-chance-significance-testing-is-misunderstood-20207) as it has many interesting links.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
